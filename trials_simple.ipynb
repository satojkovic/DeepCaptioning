{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing import sequence as keras_seq\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, RepeatVector, Embedding, LSTM, TimeDistributed, Input, Concatenate, merge\n",
    "from keras.layers import Bidirectional, Activation\n",
    "from keras.optimizers import  Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from time import time\n",
    "import pickle\n",
    "from operator import attrgetter\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from copy import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "with open('flickr8k/Flickr8k.token.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        captions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_image_descriptions(captions):\n",
    "    descriptions = defaultdict(list)\n",
    "    for cap in captions:\n",
    "        elems = cap.split('\\t')\n",
    "        fn = elems[0][:-2]\n",
    "        descriptions[fn].append(elems[1])\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = get_gt_image_descriptions(captions)\n",
    "sample_image_id = np.random.choice(list(descriptions.keys()))\n",
    "print('\\n'.join(descriptions[sample_image_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            #desc = [w.lower() for w in desc]\n",
    "            # remove punctuation\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove 'a' and 's'\n",
    "            #desc = [w for w in desc if len(w) > 1]\n",
    "            # remove tokens with numbers in them\n",
    "            #desc = [w for w in desc if w.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_descriptions = clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(cleaned_descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in cleaned_descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line)\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(cleaned_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Dev images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_FILE = 'flickr8k/Flickr_8k.trainImages.txt'\n",
    "TEST_IMAGES_FILE = 'flickr8k/Flickr_8k.testImages.txt'\n",
    "DEV_IMAGES_FILE = 'flickr8k/Flickr_8k.devImages.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_list(images_file):\n",
    "    images_list = []\n",
    "    with open(images_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            images_list.append(line)\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = get_images_list(TRAIN_IMAGES_FILE)\n",
    "test_images = get_images_list(TEST_IMAGES_FILE)\n",
    "dev_images = get_images_list(DEV_IMAGES_FILE)\n",
    "print('Num. of train images:', len(train_images))\n",
    "print('Num. of test images:', len(test_images))\n",
    "print('Num. of dev images:', len(dev_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images[0], test_images[0], dev_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_img_feats.pickle', 'rb') as f:\n",
    "    train_img_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_img_feats.keys()))\n",
    "print(train_images[0])\n",
    "print(len(train_img_feats[train_images[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_img_feats.pickle', 'rb') as f:\n",
    "    test_img_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_img_feats.keys()))\n",
    "print(test_images[0])\n",
    "print(len(test_img_feats[test_images[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 'zsosz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 'zeosz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_descriptions(images, cleaned_descriptions):\n",
    "    extracted_decriptions = {}\n",
    "    for img in images:\n",
    "        if img in cleaned_descriptions:\n",
    "            # Add EOS_TOKEN to each descriptions\n",
    "            extracted_decriptions[img] = list(map(lambda x: SOS_TOKEN + ' ' + x + ' ' + EOS_TOKEN, cleaned_descriptions[img]))\n",
    "    return extracted_decriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = extract_clean_descriptions(train_images, cleaned_descriptions)\n",
    "test_descriptions = extract_clean_descriptions(test_images, cleaned_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descriptions: train={}'.format(len(train_descriptions)))\n",
    "print('Descriptions: test={}'.format(len(test_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_image_id = np.random.choice(list(train_descriptions.keys()))\n",
    "test_sample_image_id = np.random.choice(list(test_descriptions.keys()))\n",
    "print('train sample image id:', train_sample_image_id)\n",
    "print('test sample image id:', test_sample_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<train sample description>:', train_descriptions[train_sample_image_id])\n",
    "print('<test sample description>:', test_descriptions[test_sample_image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_desc_len = 0\n",
    "for k, v in train_descriptions.items():\n",
    "    for desc in train_descriptions[k]:\n",
    "        if max_train_desc_len < len(desc.split(' ')):\n",
    "            max_train_desc_len = len(desc.split(' '))\n",
    "print(max_train_desc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flickr8k_training_dataset.txt', 'w') as f:\n",
    "    f.writelines('\\t'.join(['image_id', 'captions']))\n",
    "    f.writelines('\\n')\n",
    "    for k, v in train_descriptions.items():\n",
    "        for desc in train_descriptions[k]:\n",
    "            f.writelines('\\t'.join([k, desc]))\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = 0\n",
    "for k in train_descriptions.keys():\n",
    "    samples_per_epoch += sum([len(desc.split()) - 1 for desc in train_descriptions[k]])\n",
    "print(samples_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set()\n",
    "for k, v in train_descriptions.items():\n",
    "    for desc in train_descriptions[k]:\n",
    "        words = [d for d in desc.split(' ')]\n",
    "        for word in words:\n",
    "            train_words.add(word)\n",
    "print(len(train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size equals the number of unique words\n",
    "vocab_size = len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {val : idx for idx, val in enumerate(train_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx : val for idx, val in enumerate(train_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2idx['zsosz'])\n",
    "print(idx2word[6254])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size=32):\n",
    "    df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\n",
    "    df = df.sample(frac=1)\n",
    "    caps = []\n",
    "    imgs = []\n",
    "    for x in df.iterrows():\n",
    "        caps.append(x[1][1])\n",
    "        imgs.append(x[1][0])\n",
    "    \n",
    "    partial_caps = []\n",
    "    next_words = []\n",
    "    images = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        for j, text in enumerate(caps):\n",
    "            current_image = train_img_feats[imgs[j]]\n",
    "            for i in range(len(text.split())-1):\n",
    "                count += 1\n",
    "                partial = [word2idx[txt] for txt in text.split()[:i+1]] # \n",
    "                partial_caps.append(partial)\n",
    "                \n",
    "                n = np.zeros(vocab_size)\n",
    "                n[word2idx[text.split()[i+1]]] = 1 # setting the next word to 1 (one hot vector)\n",
    "                next_words.append(n)\n",
    "                \n",
    "                images.append(current_image)\n",
    "                \n",
    "                if count >= batch_size:\n",
    "                    next_words = np.asarray(next_words)\n",
    "                    images = np.asarray(images)\n",
    "                    partial_caps = keras_seq.pad_sequences(partial_caps, maxlen=max_train_desc_len, padding='post')\n",
    "                    yield [[images, partial_caps], next_words]\n",
    "                    partial_caps = []\n",
    "                    next_words = []\n",
    "                    images = []\n",
    "                    count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Input(shape=(2048,))\n",
    "img_dense = Dense(embedding_size)(img_input)\n",
    "image_model = RepeatVector(max_train_desc_len)(img_dense)\n",
    "print(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=[None])\n",
    "word_embed = Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_train_desc_len)(sentence_input)\n",
    "word_model = LSTM(units=256, return_sequences=True)(word_embed)\n",
    "caption_model = TimeDistributed(Dense(300))(word_model)\n",
    "print(caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_model = merge.concatenate([image_model, caption_model])\n",
    "concat_model = Bidirectional(LSTM(units=256, return_sequences=False))(concat_model)\n",
    "concat_model = Dense(vocab_size)(concat_model)\n",
    "out = Activation('softmax')(concat_model)\n",
    "final_model = Model(input=[img_input, sentence_input], output=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit_generator(data_generator(batch_size=128), samples_per_epoch=samples_per_epoch, nb_epoch=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
