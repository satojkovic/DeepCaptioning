{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "import os\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flickr8k/dataset.json', 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_data['images'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_image_descriptions(json_data):\n",
    "    descriptions = defaultdict(list)\n",
    "    for jd in json_data['images']:\n",
    "        fn = jd['filename'].split('.')[0]\n",
    "        for s in jd['sentences']:\n",
    "            descriptions[fn].append(s['raw'])\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = get_gt_image_descriptions(json_data)\n",
    "sample_image_id = np.random.choice(list(descriptions.keys()))\n",
    "print('\\n'.join(descriptions[sample_image_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [w.lower() for w in desc]\n",
    "            # remove punctuation\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove 'a' and 's'\n",
    "            desc = [w for w in desc if len(w) > 1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [w for w in desc if w.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions = clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions[sample_image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = [len(clean_descriptions[key]) for key in clean_descriptions.keys()]\n",
    "print(sum(all_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(clean_descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in clean_descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line)\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(clean_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_desc_keys, test_clean_desc_keys = train_test_split(list(clean_descriptions.keys()))\n",
    "print('train size:', len(train_clean_desc_keys))\n",
    "print('test size:', len(test_clean_desc_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_clean_descriptions(train_clean_desc_keys, filename):\n",
    "    train_clean_descriptions = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tokens = line.split()\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            # skip images not in the train set\n",
    "            if image_id in train_clean_desc_keys:\n",
    "                if not image_id in train_clean_descriptions:\n",
    "                    train_clean_descriptions[image_id] = []\n",
    "                # add start and end token\n",
    "                desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "                train_clean_descriptions[image_id].append(desc)\n",
    "    return train_clean_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = load_train_clean_descriptions(train_clean_desc_keys, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descriptions: train={}'.format(len(train_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_descriptions[sample_image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(clean_descriptions):\n",
    "    #  remove duplicate words (set of unique words)\n",
    "    vocabulary = set()\n",
    "    for key in clean_descriptions.keys():\n",
    "        [vocabulary.update(d.split()) for d in clean_descriptions[key]]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Vector Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input)\n",
    "print(model.layers[-2:])\n",
    "print(model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    # convert all images to the size 299x299 as expected by the Inception v3\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # convert PIL image to numpy array\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess image using preprocess_input from inception_v3 module\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def encode(img):\n",
    "    img = preprocess(img)\n",
    "    feat_vec = model_new.predict(img)\n",
    "    feat_vec = np.reshape(feat_vec, feat_vec.shape[1])\n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_images_filename = 'encoded_train_images.pkl'\n",
    "if not os.path.exists(encoded_train_images_filename):\n",
    "    start = time()\n",
    "    encoding_train = {}\n",
    "    for base_img_fn in train_clean_desc_keys:\n",
    "        img_fn = base_img_fn + '.jpg'\n",
    "        image_file_path = os.path.abspath(os.path.join(image_dir, img_fn))\n",
    "        if not os.path.exists(image_file_path):\n",
    "            print('Not found image:', image_file_path)\n",
    "            continue\n",
    "        encoding_train[base_img_fn] = encode(os.path.join(image_dir, img_fn))\n",
    "    print('encoding time for train:', time() - start)\n",
    "else:\n",
    "    with open(encoded_train_images_filename, 'rb') as f:\n",
    "        train_image_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_image_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_images_filename = 'encoded_test_images.pkl'\n",
    "if not os.path.exists(encoded_test_images_filename):\n",
    "    start = time()\n",
    "    encoding_test = {}\n",
    "    for i, base_img_fn in enumerate(test_clean_desc_keys):\n",
    "        img_fn = base_img_fn + '.jpg'\n",
    "        image_file_path = os.path.abspath(os.path.join(image_dir, img_fn))\n",
    "        if not os.path.exists(image_file_path):\n",
    "            print('Not found image:', image_file_path)\n",
    "            continue\n",
    "        else:\n",
    "            print('{}: {}'.format(i, img_fn))\n",
    "        encoding_test[base_img_fn] = encode(os.path.join(image_dir, img_fn))\n",
    "    print('encoding time for test:', time() - start)\n",
    "    \n",
    "    with open(encoded_test_images_filename, \"wb\") as f:\n",
    "      pickle.dump(encoding_test, f)\n",
    "else:\n",
    "    with open(encoded_test_images_filename, 'rb') as f:\n",
    "        test_image_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_image_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    if not word in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for padding\n",
    "max_id = len(word_to_id)\n",
    "word_to_id['0'] = max_id\n",
    "id_to_word[max_id] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_to_id))\n",
    "print(len(id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(clean_descriptions):\n",
    "  max_length = 0\n",
    "  for k in clean_descriptions.keys():\n",
    "    if not k in train_clean_desc_keys:\n",
    "      continue\n",
    "    max_each = max([len(desc.split()) for desc in clean_descriptions[k]])\n",
    "    if max_each > max_length:\n",
    "      max_length = max_each\n",
    "  return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max_length(train_descriptions)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "def load_word_vectors(word_vectors_path):\n",
    "    embeddings = {}\n",
    "    with open(word_vectors_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_word_vectors('glove/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num. of word vectors:', len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = model_new.layers[-1].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Sequential()\n",
    "model_encoder.add(Dense(embed_dim, input_shape=(feat_dim, )))\n",
    "model_encoder.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_encoder.layers)):  \n",
    "    print('[layer] {} : {} (input) => {} (output)'.format(model_encoder.layers[i], model_encoder.layers[i].input_shape, model_encoder.layers[i].output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
