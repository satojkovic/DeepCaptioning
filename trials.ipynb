{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing import sequence as keras_seq\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, RepeatVector, Embedding, LSTM, TimeDistributed, Input, Concatenate\n",
    "from keras.optimizers import  Adam\n",
    "import os\n",
    "from time import time\n",
    "import pickle\n",
    "from operator import attrgetter\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flickr8k/dataset.json', 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_data['images'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_image_descriptions(json_data):\n",
    "    descriptions = defaultdict(list)\n",
    "    for jd in json_data['images']:\n",
    "        fn = jd['filename'].split('.')[0]\n",
    "        for s in jd['sentences']:\n",
    "            descriptions[fn].append(s['raw'])\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = get_gt_image_descriptions(json_data)\n",
    "sample_image_id = np.random.choice(list(descriptions.keys()))\n",
    "print('\\n'.join(descriptions[sample_image_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [w.lower() for w in desc]\n",
    "            # remove punctuation\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove 'a' and 's'\n",
    "            desc = [w for w in desc if len(w) > 1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [w for w in desc if w.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions = clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions[sample_image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = [len(clean_descriptions[key]) for key in clean_descriptions.keys()]\n",
    "print(sum(all_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(clean_descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in clean_descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line)\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(clean_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_desc_keys, test_clean_desc_keys = train_test_split(list(clean_descriptions.keys()))\n",
    "print('train size:', len(train_clean_desc_keys))\n",
    "print('test size:', len(test_clean_desc_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_clean_descriptions(train_clean_desc_keys, filename):\n",
    "    train_clean_descriptions = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tokens = line.split()\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            # skip images not in the train set\n",
    "            if image_id in train_clean_desc_keys:\n",
    "                if not image_id in train_clean_descriptions:\n",
    "                    train_clean_descriptions[image_id] = []\n",
    "                # add start and end token\n",
    "                desc = ' '.join(image_desc) + ' zeosz'\n",
    "                train_clean_descriptions[image_id].append(desc)\n",
    "    return train_clean_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = load_train_clean_descriptions(train_clean_desc_keys, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descriptions: train={}'.format(len(train_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_descriptions[sample_image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(train_descriptions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([desc for k in train_descriptions.keys() for desc in train_descriptions[k]])\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_tokenizer(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 0 is reserved for padding\n",
    "vocab_size = max(tokenizer.index_word)  + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "dense_input = BatchNormalization(axis=-1)(image_model.output)\n",
    "image_dense = Dense(units=embed_dim)(dense_input)\n",
    "# Add a timestep dimension to match LSTM\n",
    "image_embedding = RepeatVector(1)(image_dense)\n",
    "image_input = image_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_input)\n",
    "print(dense_input)\n",
    "print(image_dense)\n",
    "print(image_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = image_model.output_shape[1]\n",
    "print('embed_dim {}, feat_dim {}'.format(embed_dim, feat_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=[None])\n",
    "word_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(sentence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_input)\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "def load_word_vectors(word_vectors_path):\n",
    "    embeddings = {}\n",
    "    with open(word_vectors_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word_vectors = load_word_vectors('glove/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num. of word vectors:', len(pretrained_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_embedding_matrix(tokenizer, pretrained_word_vectors, vocab_size, embed_dim):\n",
    "    word_embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # if the word is not included, get method returns None\n",
    "        word_vector = pretrained_word_vectors.get(word)\n",
    "        if word_vector is not None:\n",
    "            word_embedding_matrix[i] = word_vector\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = build_word_embedding_matrix(tokenizer, pretrained_word_vectors, vocab_size, embed_dim)\n",
    "print(word_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder CNN and Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = Concatenate(axis=1)([image_embedding, word_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_embedding)\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = seq_input\n",
    "print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 3\n",
    "lstm_output_size = embed_dim\n",
    "dropout_rate = 0.22\n",
    "for _ in range(lstm_layers):\n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    lstm_out = LSTM(\n",
    "        units=lstm_output_size, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate\n",
    "    )(input_)\n",
    "    input_ = lstm_out\n",
    "seq_output = TimeDistributed(Dense(units=vocab_size))(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt = Model(inputs=[image_input, sentence_input], outputs=seq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set weights from pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.layers[-9].set_weights([word_embedding_matrix])\n",
    "model_im2txt.layers[-9].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy_from_logits(y_true, y_pred):\n",
    "    # Discard the last timestep\n",
    "    y_true = y_true[:, :-1, :]\n",
    "    y_pred = y_pred[:, :-1, :]\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00051\n",
    "model_im2txt.compile(optimizer=Adam(lr=learning_rate), loss=categorical_crossentropy_from_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datum = namedtuple('Datum', ['img_filename', 'img_path', 'caption_txt', 'all_captions_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_datum_list(train_clean_desc_keys, train_descriptions, image_dir):\n",
    "    train_datum_list = []\n",
    "    for k in train_clean_desc_keys:\n",
    "        img_filename = k + '.jpg'\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "        all_captions_txt = train_descriptions[k]\n",
    "        for desc in all_captions_txt:\n",
    "            train_datum_list.append(Datum(img_filename=img_filename, img_path=img_path, caption_txt=desc, all_captions_txt=all_captions_txt))\n",
    "    return train_datum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datum_list = build_train_datum_list(train_clean_desc_keys, train_descriptions, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_datum_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_an_image(img_path):\n",
    "    # RGB format\n",
    "    img = image.load_img(img_path, target_size=(299, 299, 3))\n",
    "    img_array = image.img_to_array(img)\n",
    "    # Use inception_v3.preprocess_input()\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(imgs_path):\n",
    "    return map(preprocess_an_image, imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(tokenizer, captions_txt):\n",
    "    return tokenizer.texts_to_sequences(captions_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img_batch(img_batch):\n",
    "    return np.array(list(img_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption_batch(caption_batch, tokenizer):\n",
    "    # captions must have same length within a batch\n",
    "    captions = keras_seq.pad_sequences(caption_batch, padding='post')\n",
    "    # add 1 (first word is the image)\n",
    "    captions_ext1 = keras_seq.pad_sequences(captions, maxlen=captions.shape[-1] + 1, padding='post')\n",
    "    # one hot sequence (batch_size, seq_len, vocab_size)\n",
    "    captions_one_hot = map(tokenizer.sequences_to_matrix, np.expand_dims(captions_ext1, axis=-1))\n",
    "    captions_one_hot = np.array(list(captions_one_hot), dtype='int')\n",
    "    # except index 0 (i.e. remove padding index)\n",
    "    captions_one_hot_shifted = captions_one_hot[:, :, 1:]\n",
    "    # index - 1 \n",
    "    captions_decreased = captions.copy()\n",
    "    captions_decreased[captions_decreased > 0] -= 1\n",
    "    \n",
    "    captions_input = captions_decreased\n",
    "    captions_output = captions_one_hot_shifted\n",
    "    \n",
    "    return captions_input, captions_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(datum_batch, tokenizer):\n",
    "    imgs_path = map(attrgetter('img_path'), datum_batch)\n",
    "    captions_txt = map(attrgetter('caption_txt'), datum_batch)\n",
    "    img_batch = preprocess_images(imgs_path)\n",
    "    caption_batch = encode_captions(tokenizer, captions_txt)\n",
    "    img_input = preprocess_img_batch(img_batch)\n",
    "    captions = preprocess_caption_batch(caption_batch)\n",
    "    captions_input, captions_output = captions\n",
    "    X, y = [img_input, captions_input], captions_output\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch from train_datum_list\n",
    "def batch_generator(train_datum_list, batch_size):\n",
    "    datum_list = copy(train_datum_list)\n",
    "    while True:\n",
    "        np.random.shuffle(datum_list)\n",
    "        datum_batch = []\n",
    "        for datum in datum_list:\n",
    "            datum_batch.append(datum)\n",
    "            if len(datum_batch) >= batch_size:\n",
    "                yield preprocess_batch(datum_batch)\n",
    "                datum_batch = []\n",
    "        if datum_batch:\n",
    "            yield preprocess_batch(datum_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
