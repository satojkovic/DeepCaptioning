{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing import sequence as keras_seq\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, RepeatVector, Embedding, LSTM, TimeDistributed, Input, Concatenate\n",
    "from keras.optimizers import  Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from time import time\n",
    "import pickle\n",
    "from operator import attrgetter\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "with open('flickr8k/Flickr8k.token.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        captions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_image_descriptions(captions):\n",
    "    descriptions = defaultdict(list)\n",
    "    for cap in captions:\n",
    "        elems = cap.split('\\t')\n",
    "        fn = elems[0][:-2]\n",
    "        descriptions[fn].append(elems[1])\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = get_gt_image_descriptions(captions)\n",
    "sample_image_id = np.random.choice(list(descriptions.keys()))\n",
    "print('\\n'.join(descriptions[sample_image_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            #desc = [w.lower() for w in desc]\n",
    "            # remove punctuation\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove 'a' and 's'\n",
    "            #desc = [w for w in desc if len(w) > 1]\n",
    "            # remove tokens with numbers in them\n",
    "            #desc = [w for w in desc if w.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions = clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(clean_descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in clean_descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line)\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(clean_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Dev images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_FILE = 'flickr8k/Flickr_8k.trainImages.txt'\n",
    "TEST_IMAGES_FILE = 'flickr8k/Flickr_8k.testImages.txt'\n",
    "DEV_IMAGES_FILE = 'flickr8k/Flickr_8k.devImages.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_list(images_file):\n",
    "    images_list = []\n",
    "    with open(images_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            images_list.append(line)\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = get_images_list(TRAIN_IMAGES_FILE)\n",
    "test_images = get_images_list(TEST_IMAGES_FILE)\n",
    "dev_images = get_images_list(DEV_IMAGES_FILE)\n",
    "print('Num. of train images:', len(train_images))\n",
    "print('Num. of test images:', len(test_images))\n",
    "print('Num. of dev images:', len(dev_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images[0], test_images[0], dev_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 'zeosz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_descriptions(images, clean_descriptions):\n",
    "    extracted_decriptions = {}\n",
    "    for img in images:\n",
    "        if img in clean_descriptions:\n",
    "            # Add EOS_TOKEN to each descriptions\n",
    "            extracted_decriptions[img] = list(map(lambda x: x + ' ' + EOS_TOKEN, clean_descriptions[img]))\n",
    "    return extracted_decriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = extract_clean_descriptions(train_images, clean_descriptions)\n",
    "test_descriptions = extract_clean_descriptions(test_images, clean_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descriptions: train={}'.format(len(train_descriptions)))\n",
    "print('Descriptions: test={}'.format(len(test_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_image_id = np.random.choice(list(train_descriptions.keys()))\n",
    "test_sample_image_id = np.random.choice(list(test_descriptions.keys()))\n",
    "print('train sample image id:', train_sample_image_id)\n",
    "print('test sample image id:', test_sample_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<train sample description>:', train_descriptions[train_sample_image_id])\n",
    "print('<test sample description>:', test_descriptions[test_sample_image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_desc_len = 0\n",
    "for k, v in train_descriptions.items():\n",
    "    for desc in train_descriptions[k]:\n",
    "        if max_train_desc_len < len(desc.split(' ')):\n",
    "            max_train_desc_len = len(desc.split(' '))\n",
    "print(max_train_desc_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(train_descriptions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([desc for k in train_descriptions.keys() for desc in train_descriptions[k]])\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_tokenizer(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_of = {i: w for w, i in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 0 is reserved for padding\n",
    "vocab_size_no_pad = max(tokenizer.index_word)\n",
    "vocab_size_with_pad = vocab_size_no_pad + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary size(no padding)', vocab_size_no_pad)\n",
    "print('vocabulary size(with padding):', vocab_size_with_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "dense_input = BatchNormalization(axis=-1)(image_model.output)\n",
    "image_dense = Dense(units=embed_dim)(dense_input)\n",
    "# Add a timestep dimension to match LSTM\n",
    "image_embedding = RepeatVector(1)(image_dense)\n",
    "image_input = image_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_input)\n",
    "print(dense_input)\n",
    "print(image_dense)\n",
    "print(image_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = image_model.output_shape[1]\n",
    "print('embed_dim {}, feat_dim {}'.format(embed_dim, feat_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=[None])\n",
    "word_embedding = Embedding(input_dim=vocab_size_no_pad, output_dim=embed_dim)(sentence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_input)\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "def load_word_vectors(word_vectors_path):\n",
    "    embeddings = {}\n",
    "    with open(word_vectors_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word_vectors = load_word_vectors('glove/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num. of word vectors:', len(pretrained_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_embedding_matrix(tokenizer, pretrained_word_vectors, vocab_size_no_pad, embed_dim):\n",
    "    word_embedding_matrix = np.zeros((vocab_size_no_pad, embed_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # if the word is not included, get method returns None\n",
    "        word_vector = pretrained_word_vectors.get(word)\n",
    "        if word_vector is not None:\n",
    "            word_embedding_matrix[i - 1] = word_vector\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = build_word_embedding_matrix(tokenizer, pretrained_word_vectors, vocab_size_no_pad, embed_dim)\n",
    "print(word_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder CNN and Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = Concatenate(axis=1)([image_embedding, word_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_embedding)\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = seq_input\n",
    "print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 3\n",
    "lstm_output_size = embed_dim\n",
    "dropout_rate = 0.22\n",
    "for _ in range(lstm_layers):\n",
    "    input_ = BatchNormalization(axis=-1)(input_)\n",
    "    lstm_out = LSTM(\n",
    "        units=lstm_output_size, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate\n",
    "    )(input_)\n",
    "    input_ = lstm_out\n",
    "seq_output = TimeDistributed(Dense(units=vocab_size_no_pad))(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt = Model(inputs=[image_input, sentence_input], outputs=seq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set weights from pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.layers[-9].set_weights([word_embedding_matrix])\n",
    "model_im2txt.layers[-9].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy_from_logits(y_true, y_pred):\n",
    "    # Discard the last timestep\n",
    "    y_true = y_true[:, :-1, :]\n",
    "    y_pred = y_pred[:, :-1, :]\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy_with_variable_timestep(y_true, y_pred):\n",
    "    # Discard the last timestep\n",
    "    y_true = y_true[:, :-1, :]\n",
    "    y_pred = y_pred[:, :-1 :]\n",
    "    # Flatten the timestep dimension\n",
    "    shape = tf.shape(y_true)\n",
    "    y_true = tf.reshape(y_true, [-1, shape[-1]])\n",
    "    y_pred = tf.reshape(y_pred, [-1, shape[-1]])\n",
    "    # Discard rows that are all zeros as they represent padding words\n",
    "    is_zero_y_true = tf.equal(y_true, 0)\n",
    "    is_zero_row_y_true = tf.reduce_all(is_zero_y_true, axis=-1)\n",
    "    y_true = tf.boolean_mask(y_true, ~is_zero_row_y_true)\n",
    "    y_pred = tf.boolean_mask(y_pred, ~is_zero_row_y_true)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_pred, axis=1)), dtype=tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00051\n",
    "model_im2txt.compile(optimizer=Adam(lr=learning_rate), loss=categorical_crossentropy_from_logits, metrics=[categorical_accuracy_with_variable_timestep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'flickr8k/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datum = namedtuple('Datum', ['img_filename', 'img_path', 'caption_txt', 'all_captions_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datum_list(clean_desc_keys, descriptions, image_dir):\n",
    "    datum_list = []\n",
    "    for k in clean_desc_keys:\n",
    "        img_filename = k + '.jpg'\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "        all_captions_txt = descriptions[k]\n",
    "        for desc in all_captions_txt:\n",
    "            datum_list.append(Datum(img_filename=img_filename, img_path=img_path, caption_txt=desc, all_captions_txt=all_captions_txt))\n",
    "    return datum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datum_list = build_datum_list(train_clean_desc_keys, train_descriptions, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_datum_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_an_image(img_path):\n",
    "    # RGB format\n",
    "    img = image.load_img(img_path, target_size=(299, 299, 3))\n",
    "    img_array = image.img_to_array(img)\n",
    "    # Use inception_v3.preprocess_input()\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(imgs_path):\n",
    "    return map(preprocess_an_image, imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(tokenizer, captions_txt):\n",
    "    return tokenizer.texts_to_sequences(captions_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img_batch(img_batch):\n",
    "    return np.array(list(img_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption_batch(caption_batch, tokenizer):\n",
    "    # captions must have same length within a batch\n",
    "    captions = keras_seq.pad_sequences(caption_batch, padding='post')\n",
    "    # add 1 (first word is the image)\n",
    "    captions_ext1 = keras_seq.pad_sequences(captions, maxlen=captions.shape[-1] + 1, padding='post')\n",
    "    # one hot sequence (batch_size, seq_len, vocab_size)\n",
    "    captions_one_hot = map(tokenizer.sequences_to_matrix, np.expand_dims(captions_ext1, axis=-1))\n",
    "    captions_one_hot = np.array(list(captions_one_hot), dtype='int')\n",
    "    # except index 0 (i.e. remove padding index)\n",
    "    captions_one_hot_shifted = captions_one_hot[:, :, 1:]\n",
    "    # index - 1  (index 0 is 'zeosz')\n",
    "    captions_decreased = captions.copy()\n",
    "    captions_decreased[captions_decreased > 0] -= 1\n",
    "    \n",
    "    captions_input = captions_decreased\n",
    "    captions_output = captions_one_hot_shifted\n",
    "    \n",
    "    return captions_input, captions_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(datum_batch, tokenizer):\n",
    "    imgs_path = map(attrgetter('img_path'), datum_batch)\n",
    "    captions_txt = map(attrgetter('caption_txt'), datum_batch)\n",
    "    img_batch = preprocess_images(imgs_path)\n",
    "    caption_batch = encode_captions(tokenizer, captions_txt)\n",
    "    img_input = preprocess_img_batch(img_batch)\n",
    "    captions = preprocess_caption_batch(caption_batch, tokenizer)\n",
    "    captions_input, captions_output = captions\n",
    "    X, y = [img_input, captions_input], captions_output\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch from train_datum_list or test_datum_list\n",
    "def batch_generator(datum_list, batch_size, tokenizer):\n",
    "    datum_list_c = copy(datum_list)\n",
    "    while True:\n",
    "        np.random.shuffle(datum_list_c)\n",
    "        datum_batch = []\n",
    "        for datum in datum_list_c:\n",
    "            datum_batch.append(datum)\n",
    "            if len(datum_batch) >= batch_size:\n",
    "                yield preprocess_batch(datum_batch, tokenizer)\n",
    "                datum_batch = []\n",
    "        if datum_batch:\n",
    "            yield preprocess_batch(datum_batch, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = len(train_descriptions)\n",
    "print(training_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = int(math.ceil(1. * training_set_size / batch_size))\n",
    "print(training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "max_q_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set(train_datum_list, batch_size, tokenizer):\n",
    "    for batch in batch_generator(train_datum_list, batch_size, tokenizer):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'weights.epoch{epoch:02d}-loss{loss:.2f}-acc{categorical_accuracy_with_variable_timestep:.2f}-.hdf5'\n",
    "cp_cb = ModelCheckpoint(filepath=fpath, save_best_only=False, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.fit_generator(generator=training_set(train_datum_list, batch_size, tokenizer), steps_per_epoch=training_steps, epochs=epochs, max_queue_size=max_q_size, verbose=1, callbacks=[cp_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_im2txt.load_weights('weights.epoch96-loss1.74-acc0.34-.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = os.path.join(image_dir, test_sample_image_id + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.load_img(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datum_list = build_datum_list(test_clean_desc_keys, test_descriptions, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_datum_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(test_datum_list, batch_size, tokenizer):\n",
    "    for batch in batch_generator(test_datum_list, batch_size, tokenizer):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_captions(captions_pred):    \n",
    "    captions = captions_pred[:, :-1, :]  # discard the last word\n",
    "    label_encoded = captions.argmax(axis=-1)\n",
    "    num_batches, num_words = label_encoded.shape\n",
    "    caption_length = [num_words] * num_batches\n",
    "    \n",
    "    captions_str = []\n",
    "    for caption_i in range(num_batches):\n",
    "        caption_str = []\n",
    "        for word_i in range(caption_length[caption_i]):\n",
    "            label = label_encoded[caption_i, word_i]\n",
    "            label += 1  # Real label = label in model + 1\n",
    "            caption_str.append(word_of[label])\n",
    "        captions_str.append(caption_str)\n",
    "    \n",
    "    return captions_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_inference(X, model_im2txt, tokenizer, max_caption_length=20):\n",
    "    captions_pred = model_im2txt.predict_on_batch(X)\n",
    "    captions_pred_str = decode_captions(captions_pred)\n",
    "    return captions_pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = test_set(test_datum_list, 1, tokenizer)\n",
    "batch_input, batch_output = it.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inference\n",
    "captions_pred_str = basic_inference(batch_input, model_im2txt, tokenizer)\n",
    "print(' '.join(captions_pred_str[0]))\n",
    "print(' '.join(decode_captions(batch_output)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predict_on_batch(X, model_im2txt, tokenizer, max_caption_length=20):\n",
    "    imgs_input, _ = X\n",
    "    batch_size = imgs_input.shape[0]\n",
    "    EOS_ENCODED = tokenizer.word_index(EOS_TOKEN)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
