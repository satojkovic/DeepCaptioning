{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flickr8k/dataset.json', 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['images'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_data['images'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_image_descriptions(json_data):\n",
    "    descriptions = defaultdict(list)\n",
    "    for jd in json_data['images']:\n",
    "        fn = jd['filename'].split('.')[0]\n",
    "        for s in jd['sentences']:\n",
    "            descriptions[fn].append(s['raw'])\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = get_gt_image_descriptions(json_data)\n",
    "sample_image_id = np.random.choice(list(descriptions.keys()))\n",
    "print('\\n'.join(descriptions[sample_image_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [w.lower() for w in desc]\n",
    "            # remove punctuation\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove 'a' and 's'\n",
    "            desc = [w for w in desc if len(w) > 1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [w for w in desc if w.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions = clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_descriptions[sample_image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = [len(clean_descriptions[key]) for key in clean_descriptions.keys()]\n",
    "print(sum(all_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(clean_descriptions):\n",
    "    #  remove duplicate words (set of unique words)\n",
    "    vocabulary = set()\n",
    "    for key in clean_descriptions.keys():\n",
    "        [vocabulary.update(d.split()) for d in clean_descriptions[key]]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(clean_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(clean_descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in clean_descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line)\n",
    "            f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(clean_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_desc_keys, test_clean_desc_keys = train_test_split(list(clean_descriptions.keys()))\n",
    "print('train size:', len(train_clean_desc_keys))\n",
    "print('test size:', len(test_clean_desc_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_clean_descriptions(train_clean_desc_keys, filename):\n",
    "    train_clean_descriptions = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tokens = line.split()\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            # skip images not in the train set\n",
    "            if image_id in train_clean_desc_keys:\n",
    "                if not image_id in train_clean_descriptions:\n",
    "                    train_clean_descriptions[image_id] = []\n",
    "                # add start and end token\n",
    "                desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "                train_clean_descriptions[image_id].append(desc)\n",
    "    return train_clean_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = load_train_clean_descriptions(train_clean_desc_keys, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descriptions: train={}'.format(len(train_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_descriptions[sample_image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
